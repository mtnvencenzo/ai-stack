# Global
TZ=UTC

# Open WebUI
OPENWEBUI_PORT=3000

# Ollama
OLLAMA_PORT=11434
# Preload models by running: docker compose exec ollama ollama pull llama3.1:8b

# Qdrant
QDRANT_HTTP_PORT=6333
QDRANT_GRPC_PORT=6334

# Embeddings server (HF Text Embeddings Inference)
TEI_PORT=8080
TEI_MODEL_ID=BAAI/bge-small-en-v1.5
TEI_MAX_CLIENT_BATCH=128
# Optional, speeds up downloads and grants access to gated models
HF_TOKEN=

# MLflow
MLFLOW_PORT=5000

# Redis (change to 6379 if free; defaults to 6380 to avoid conflicts with local Redis)
REDIS_PORT=6380

# Jupyter
JUPYTER_PORT=8888
JUPYTER_TOKEN=ai-stack

# Prefect (optional profile "orchestration")
PREFECT_PORT=4200

# vLLM (optional profile "gpu")
VLLM_PORT=8000
VLLM_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
VLLM_DTYPE=float16
VLLM_MAXLEN=8192
VLLM_GPU_MEM_UTIL=0.90

# LangFlow (optional profile "studio")
LANGFLOW_PORT=7860
